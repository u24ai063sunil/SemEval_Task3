{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DimABSA 2026 – Subtask 1 (DimASR)\n",
    "## Chinese Laptop (zho_laptop) – 4-Epoch PyTorch Baseline\n",
    "\n",
    "This notebook trains a multilingual DistilBERT regression model for **DimASR** on the **Chinese Laptop** dataset:\n",
    "\n",
    "- Train: `Chi_laptop_train_alltasks.jsonl`  (Quadruplet format)\n",
    "- Dev:   `Chi_laptop_dev_task1.jsonl`      (Aspect list format)\n",
    "\n",
    "It outputs predictions in the required JSONL format for **Subtask 1** and saves them as:\n",
    "\n",
    "```text\n",
    "pred_zho_laptop.jsonl\n",
    "```\n",
    "\n",
    "Place this file inside `subtask_1/` as `pred_zho_laptop.jsonl` for Codabench submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Working directory: c:\\SemEval_task3\\datasets\\Chi\n",
      "Files: ['Chi_finance_dev_task1.jsonl', 'Chi_finance_task1_notebook.ipynb', 'Chi_finance_train_task1.jsonl', 'Chi_laptop_dev_task1.jsonl', 'Chi_laptop_dev_task2.jsonl', 'Chi_laptop_dev_task3.jsonl', 'Chi_laptop_task1_notebook.ipynb', 'Chi_laptop_train_alltasks.jsonl', 'Chi_restaurant_dev_task1.jsonl', 'Chi_restaurant_dev_task2.jsonl', 'Chi_restaurant_dev_task3.jsonl', 'Chi_restaurant_train_alltasks.jsonl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Working directory:', os.getcwd())\n",
    "print('Files:', os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi_laptop_train_alltasks.jsonl -> 3490 records\n",
      "Chi_laptop_dev_task1.jsonl -> 261 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN = 'Chi_laptop_train_alltasks.jsonl'\n",
    "DEV   = 'Chi_laptop_dev_task1.jsonl'\n",
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    print(f'{path} -> {len(data)} records')\n",
    "    return data\n",
    "\n",
    "train_json = read_jsonl(TRAIN)\n",
    "dev_json   = read_jsonl(DEV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN rows: 6502\n",
      "DEV rows: 431\n",
      "Train DF head:\n",
      "             ID                          Text   Aspect  valence  arousal\n",
      "0  6700135:S006  恭喜入手新機，這台筆電規格不錯又輕巧，可惜螢幕只有FHD       規格     6.00     5.00\n",
      "1  6700135:S006  恭喜入手新機，這台筆電規格不錯又輕巧，可惜螢幕只有FHD  螢幕只有FHD     4.17     5.00\n",
      "2  6700135:S006  恭喜入手新機，這台筆電規格不錯又輕巧，可惜螢幕只有FHD       筆電     6.17     5.00\n",
      "3  6699557:S020       重量真的輕，鍵盤分離還可開啟藍芽使用也很方便。       重量     6.83     6.00\n",
      "4  6699557:S020       重量真的輕，鍵盤分離還可開啟藍芽使用也很方便。       鍵盤     5.75     4.75\n",
      "\n",
      "Dev DF head:\n",
      "             ID                     Text  Aspect\n",
      "0  6952865:S310  ASUS筆電總是不會讓人失望！AI的時代來了！  ASUS筆電\n",
      "1  6949617:S106   技嘉NB之前用過一次覺得還不錯~CP值算高!    技嘉NB\n",
      "2  6949617:S106   技嘉NB之前用過一次覺得還不錯~CP值算高!     CP值\n",
      "3  6931799:S046          筆電跟MD都很完美，五分奉上~      筆電\n",
      "4  6946311:S117      鍵盤和觸控板設計與美感頗具吸引力...      鍵盤\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train DF from Quadruplet format\n",
    "def build_train_df(data):\n",
    "    rows = []\n",
    "    for ex in data:\n",
    "        tid = ex['ID']\n",
    "        text = ex['Text']\n",
    "        for q in ex.get('Quadruplet', []):\n",
    "            try:\n",
    "                v_str, a_str = q['VA'].split('#')\n",
    "                rows.append({\n",
    "                    'ID': tid,\n",
    "                    'Text': text,\n",
    "                    'Aspect': q['Aspect'],\n",
    "                    'valence': float(v_str),\n",
    "                    'arousal': float(a_str),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print('Skipping invalid VA in train:', q.get('VA'), 'error:', e)\n",
    "    print('TRAIN rows:', len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Dev DF from Aspect list\n",
    "def build_dev_df(data):\n",
    "    rows = []\n",
    "    for ex in data:\n",
    "        tid = ex['ID']\n",
    "        text = ex['Text']\n",
    "        for asp in ex['Aspect']:\n",
    "            rows.append({\n",
    "                'ID': tid,\n",
    "                'Text': text,\n",
    "                'Aspect': asp,\n",
    "            })\n",
    "    print('DEV rows:', len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = build_train_df(train_json)\n",
    "dev_df   = build_dev_df(dev_json)\n",
    "\n",
    "print('Train DF head:')\n",
    "print(train_df.head())\n",
    "print('\\nDev DF head:')\n",
    "print(dev_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def combine_text(text, aspect):\n",
    "    return f\"{text} [ASP] {aspect}\"\n",
    "\n",
    "class DimASRDataset(Dataset):\n",
    "    def __init__(self, df, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        combined = combine_text(row['Text'], row['Aspect'])\n",
    "        enc = tokenizer(\n",
    "            combined,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['ID'] = row['ID']\n",
    "        item['Aspect'] = row['Aspect']\n",
    "        if self.is_train:\n",
    "            item['labels'] = torch.tensor(\n",
    "                [row['valence'], row['arousal']],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        return item\n",
    "\n",
    "train_ds = DimASRDataset(train_df, is_train=True)\n",
    "dev_ds   = DimASRDataset(dev_df,   is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 813\n",
      "Dev batches: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids = [x['ID'] for x in batch]\n",
    "    aspects = [x['Aspect'] for x in batch]\n",
    "    for x in batch:\n",
    "        x.pop('ID')\n",
    "        x.pop('Aspect')\n",
    "    padded = collator(batch)\n",
    "    padded['ID'] = ids\n",
    "    padded['Aspect'] = aspects\n",
    "    return padded\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('Train batches:', len(train_loader))\n",
    "print('Dev batches:', len(dev_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DimASRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        self.reg  = nn.Linear(768, 2)  # valence, arousal\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        return self.reg(cls)\n",
    "\n",
    "model = DimASRModel().to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167db3acc6aa4d2aa69f13cd3c03794e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 1.2178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d89fbef4d491a81b5cdd8b12f0d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 0.4872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfb191a5a3046faace72264f9db25b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 0.3411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 3\n",
    "print('Starting training for', EPOCHS, 'epochs...')\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {ep+1}/{EPOCHS}'):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        y    = batch['labels'].to(device)\n",
    "\n",
    "        preds = model(ids, mask)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "    print(f'Epoch {ep+1} average loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on dev set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe420a57adfd4a64a2af0b68d6d20486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Running inference on dev set...')\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc='Inference'):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        logits = model(ids, mask).cpu().numpy()\n",
    "\n",
    "        for i, (ID, asp) in enumerate(zip(batch['ID'], batch['Aspect'])):\n",
    "            v, a = logits[i]\n",
    "            preds.append((ID, asp, f\"{v:.2f}#{a:.2f}\"))\n",
    "\n",
    "print('Total predictions:', len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to pred_zho_laptop.jsonl\n",
      "Exists: True\n",
      "Size (bytes): 30087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT = 'pred_zho_laptop.jsonl'\n",
    "\n",
    "sub = {}\n",
    "for ID, asp, va in preds:\n",
    "    sub.setdefault(ID, []).append({'Aspect': asp, 'VA': va})\n",
    "\n",
    "with open(OUT, 'w', encoding='utf8') as f:\n",
    "    for ex in dev_json:\n",
    "        rec = {\n",
    "            'ID': ex['ID'],\n",
    "            'Aspect_VA': sub.get(ex['ID'], [])\n",
    "        }\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Saved predictions to', OUT)\n",
    "print('Exists:', os.path.exists(OUT))\n",
    "print('Size (bytes):', os.path.getsize(OUT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
