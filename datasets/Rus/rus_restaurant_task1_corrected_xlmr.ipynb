{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian Restaurant – DimASR (Subtask 1) with XLM-RoBERTa\n",
    "This notebook trains an `xlm-roberta-base` regression model for Russian restaurant data.\n",
    "- Train: `rus_restaurant_train_alltasks.jsonl` (Quadruplet format)\n",
    "- Dev:   `rus_restaurant_dev_task1.jsonl`     (Aspect list)\n",
    "It outputs predictions in `pred_rus_restaurant.jsonl` ready for SemEval Subtask 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "\n",
    "TRAIN = 'rus_restaurant_train_alltasks.jsonl'\n",
    "DEV   = 'rus_restaurant_dev_task1.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus_restaurant_train_alltasks.jsonl -> 1240 records\n",
      "rus_restaurant_dev_task1.jsonl -> 56 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    print(path, '->', len(data), 'records')\n",
    "    return data\n",
    "\n",
    "train_json = read_jsonl(TRAIN)\n",
    "dev_json   = read_jsonl(DEV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN rows: 2487\n",
      "DEV rows: 81\n",
      "Train DF head:\n",
      "         ID                                               Text        Aspect  \\\n",
      "0   225:3_0                 Виды из окна - выше всяких похвал.  Виды из окна   \n",
      "1   225:4_1                            Интерьер не запомнился.      Интерьер   \n",
      "2  225:10_2  Меню показалось немного скучным, но мы быстро ...          Меню   \n",
      "3  225:13_3  Мясо было прожарено чуть сильнее,чем заказывал...          Мясо   \n",
      "4  225:13_3  Мясо было прожарено чуть сильнее,чем заказывал...          Мясо   \n",
      "\n",
      "   valence  arousal  \n",
      "0     8.30     7.60  \n",
      "1     5.00     3.20  \n",
      "2     4.62     3.88  \n",
      "3     7.00     6.20  \n",
      "4     6.90     6.20  \n",
      "\n",
      "Dev DF head:\n",
      "         ID                                               Text   Aspect\n",
      "0  354:12_0  Блюдо хинкали состоит из 4 штук, хороших разме...  хинкали\n",
      "1  1180:1_1  ОЧЕНЬ все понравилось, музыка ненавязчивая, де...   музыка\n",
      "2  1180:1_1  ОЧЕНЬ все понравилось, музыка ненавязчивая, де...  девушка\n",
      "3  1180:1_1  ОЧЕНЬ все понравилось, музыка ненавязчивая, де...   порции\n",
      "4  1206:8_2  Плюс блюда вполне понятны(я бы сказал чуть по ...    блюда\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_train_df(data):\n",
    "    rows = []\n",
    "    for ex in data:\n",
    "        tid = ex.get('ID')\n",
    "        text = ex.get('Text', '')\n",
    "        # Prefer Quadruplet if available\n",
    "        quads = ex.get('Quadruplet', [])\n",
    "        if quads:\n",
    "            for q in quads:\n",
    "                va = q.get('VA')\n",
    "                asp = q.get('Aspect')\n",
    "                if not va or asp is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    v_str, a_str = va.split('#')\n",
    "                    rows.append({\n",
    "                        'ID': tid,\n",
    "                        'Text': text,\n",
    "                        'Aspect': asp,\n",
    "                        'valence': float(v_str),\n",
    "                        'arousal': float(a_str),\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print('Skipping invalid VA:', va, 'error:', e)\n",
    "        else:\n",
    "            # Fallback: some lines may have single Category/VA but no Quadruplet\n",
    "            va = ex.get('VA')\n",
    "            asp = ex.get('Aspect')\n",
    "            if va and asp:\n",
    "                try:\n",
    "                    v_str, a_str = va.split('#')\n",
    "                    rows.append({\n",
    "                        'ID': tid,\n",
    "                        'Text': text,\n",
    "                        'Aspect': asp,\n",
    "                        'valence': float(v_str),\n",
    "                        'arousal': float(a_str),\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print('Skipping top-level VA:', va, 'error:', e)\n",
    "    print('TRAIN rows:', len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_dev_df(data):\n",
    "    rows = []\n",
    "    for ex in data:\n",
    "        tid = ex['ID']\n",
    "        text = ex['Text']\n",
    "        for asp in ex['Aspect']:\n",
    "            rows.append({\n",
    "                'ID': tid,\n",
    "                'Text': text,\n",
    "                'Aspect': asp\n",
    "            })\n",
    "    print('DEV rows:', len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = build_train_df(train_json)\n",
    "dev_df   = build_dev_df(dev_json)\n",
    "\n",
    "print('Train DF head:')\n",
    "print(train_df.head())\n",
    "print('\\nDev DF head:')\n",
    "print(dev_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = 'xlm-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def combine(text, aspect):\n",
    "    return f\"{text} [ASP] {aspect}\"\n",
    "\n",
    "class RusRestaurantDataset(Dataset):\n",
    "    def __init__(self, df, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        combined = combine(row['Text'], row['Aspect'])\n",
    "        enc = tokenizer(\n",
    "            combined,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['ID'] = row['ID']\n",
    "        item['Aspect'] = row['Aspect']\n",
    "        if self.is_train:\n",
    "            item['labels'] = torch.tensor(\n",
    "                [row['valence'], row['arousal']],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        return item\n",
    "\n",
    "train_ds = RusRestaurantDataset(train_df, is_train=True)\n",
    "dev_ds   = RusRestaurantDataset(dev_df,   is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 311\n",
      "Dev batches: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids = [x['ID'] for x in batch]\n",
    "    aspects = [x['Aspect'] for x in batch]\n",
    "    for x in batch:\n",
    "        x.pop('ID')\n",
    "        x.pop('Aspect')\n",
    "    padded = collator(batch)\n",
    "    padded['ID'] = ids\n",
    "    padded['Aspect'] = aspects\n",
    "    return padded\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('Train batches:', len(train_loader))\n",
    "print('Dev batches:', len(dev_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DimASRRusRestaurantModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(MODEL)\n",
    "        hidden = self.base.config.hidden_size\n",
    "        self.reg = nn.Linear(hidden, 2)  # valence, arousal\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        return self.reg(cls)\n",
    "\n",
    "model = DimASRRusRestaurantModel().to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 4 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b9aa361dbc443396dda60468dd916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/4:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 5.2418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89900c2463c475e95ada5e25b2bb783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/4:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 1.6595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fc40c22fd5448cb6fb1973bf63e4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/4:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 1.1681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6143df4b0ddf442d85dcb4511380ae50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/4:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss: 0.9563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 4\n",
    "print('Starting training for', EPOCHS, 'epochs...')\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {ep+1}/{EPOCHS}'):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        y    = batch['labels'].to(device)\n",
    "\n",
    "        preds = model(ids, mask)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "    print(f'Epoch {ep+1} average loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on dev set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0e1554addc464ca5b208d2e008f33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Running inference on dev set...')\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc='Inference'):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        logits = model(ids, mask).cpu().numpy()\n",
    "\n",
    "        for i, (ID, asp) in enumerate(zip(batch['ID'], batch['Aspect'])):\n",
    "            v, a = logits[i]\n",
    "            preds.append((ID, asp, f\"{v:.2f}#{a:.2f}\"))\n",
    "\n",
    "print('Total predictions:', len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to pred_rus_restaurant.jsonl\n",
      "Exists: True\n",
      "Size (bytes): 6284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT = 'pred_rus_restaurant.jsonl'\n",
    "sub = {}\n",
    "\n",
    "for ID, asp, va in preds:\n",
    "    sub.setdefault(ID, []).append({'Aspect': asp, 'VA': va})\n",
    "\n",
    "with open(OUT, 'w', encoding='utf8') as f:\n",
    "    for ex in dev_json:\n",
    "        rec = {\n",
    "            'ID': ex['ID'],\n",
    "            'Aspect_VA': sub.get(ex['ID'], [])\n",
    "        }\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Saved predictions to', OUT)\n",
    "print('Exists:', os.path.exists(OUT))\n",
    "print('Size (bytes):', os.path.getsize(OUT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
