{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e78663c",
   "metadata": {},
   "source": [
    "# DimABSA Subtask1 â€” PyTorch Training (with padding + correct dev parsing + debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f35e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Working dir: c:\\SemEval_task3\\datasets\\eng\n",
      "Files: ['1_e_res.ipynb', 'DimABSA_PyTorch_Debug.ipynb', 'eng_laptop_dev_task1.jsonl', 'eng_laptop_dev_task2.jsonl', 'eng_laptop_dev_task3.jsonl', 'eng_laptop_train_alltasks.jsonl', 'eng_restaurant_dev_task1.jsonl', 'eng_restaurant_dev_task2.jsonl', 'eng_restaurant_dev_task3.jsonl', 'eng_restaurant_train_alltasks.jsonl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Working dir:', os.getcwd())\n",
    "print('Files:', os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6814c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_laptop_train_alltasks.jsonl -> records: 4076\n",
      "eng_laptop_dev_task1.jsonl -> records: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN='eng_laptop_train_alltasks.jsonl'\n",
    "DEV='eng_laptop_dev_task1.jsonl'\n",
    "\n",
    "def read_jsonl(p):\n",
    "    out=[]\n",
    "    with open(p,'r',encoding='utf8') as f:\n",
    "        for l in f:\n",
    "            l=l.strip()\n",
    "            if l:\n",
    "                out.append(json.loads(l))\n",
    "    print(p, '-> records:', len(out))\n",
    "    return out\n",
    "\n",
    "train_json=read_jsonl(TRAIN)\n",
    "dev_json=read_jsonl(DEV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ba0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN rows: 5773\n",
      "DEV rows: 275\n",
      "                  ID                                               Text  \\\n",
      "0  laptop_quad_dev_1  this unit is ` ` pretty ` ` and stylish , so m...   \n",
      "1  laptop_quad_dev_1  this unit is ` ` pretty ` ` and stylish , so m...   \n",
      "2  laptop_quad_dev_2  for now i ' m okay with upping the experience ...   \n",
      "3  laptop_quad_dev_3  seems unlikely but whatever , i ' ll go with it .   \n",
      "4  laptop_quad_dev_4  this version has been my least favorite versio...   \n",
      "\n",
      "    Aspect  valence  arousal  \n",
      "0     unit     7.12     7.12  \n",
      "1     unit     7.12     7.12  \n",
      "2   device     5.50     5.25  \n",
      "3     NULL     5.00     5.12  \n",
      "4  version     3.30     6.60  \n",
      "                      ID                                               Text  \\\n",
      "0  lap26_aspect_va_dev_1                    The touchscreen works very well   \n",
      "1  lap26_aspect_va_dev_2                         I am so disappointed in HP   \n",
      "2  lap26_aspect_va_dev_3  The keyboard is big enough to use for real typing   \n",
      "3  lap26_aspect_va_dev_4                             I like the screen size   \n",
      "4  lap26_aspect_va_dev_5            Lenovo is my favorite brand of computer   \n",
      "\n",
      "        Aspect  \n",
      "0  touchscreen  \n",
      "1           HP  \n",
      "2     keyboard  \n",
      "3  screen size  \n",
      "4       Lenovo  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TRAIN DF (from Quadruplets)\n",
    "def build_train_df(data):\n",
    "    rows=[]\n",
    "    for ex in data:\n",
    "        tid=ex['ID']; text=ex['Text']\n",
    "        for q in ex.get(\"Quadruplet\",[]):\n",
    "            v,a=q['VA'].split('#')\n",
    "            rows.append({\n",
    "                'ID':tid,'Text':text,'Aspect':q['Aspect'],\n",
    "                'valence':float(v),'arousal':float(a)\n",
    "            })\n",
    "    print(\"TRAIN rows:\", len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df=build_train_df(train_json)\n",
    "\n",
    "# DEV DF (from Aspect list)\n",
    "def build_dev_df(data):\n",
    "    rows=[]\n",
    "    for ex in data:\n",
    "        for asp in ex[\"Aspect\"]:\n",
    "            rows.append({\n",
    "                \"ID\":ex[\"ID\"],\n",
    "                \"Text\":ex[\"Text\"],\n",
    "                \"Aspect\":asp\n",
    "            })\n",
    "    print(\"DEV rows:\", len(rows))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "dev_df=build_dev_df(dev_json)\n",
    "\n",
    "print(train_df.head())\n",
    "print(dev_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00100a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def combine(t,a): return f\"{t} [ASP] {a}\"\n",
    "\n",
    "class DimSet(Dataset):\n",
    "    def __init__(self,df,train=True):\n",
    "        self.df=df; self.train=train\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        r=self.df.iloc[idx]\n",
    "        enc=tokenizer(combine(r[\"Text\"],r[\"Aspect\"]),\n",
    "                      truncation=True,max_length=128,return_tensors='pt')\n",
    "        out={k:v.squeeze(0) for k,v in enc.items()}\n",
    "        out[\"ID\"]=r[\"ID\"]; out[\"Aspect\"]=r[\"Aspect\"]\n",
    "        if self.train:\n",
    "            out[\"labels\"]=torch.tensor([r[\"valence\"],r[\"arousal\"]],dtype=torch.float32)\n",
    "        return out\n",
    "\n",
    "train_ds=DimSet(train_df,True)\n",
    "dev_ds=DimSet(dev_df,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c2e643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 722\n",
      "Dev batches: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collator=DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "def collate(batch):\n",
    "    IDs=[x[\"ID\"] for x in batch]\n",
    "    ASP=[x[\"Aspect\"] for x in batch]\n",
    "    for x in batch:\n",
    "        x.pop(\"ID\"); x.pop(\"Aspect\")\n",
    "    pad=collator(batch)\n",
    "    pad[\"ID\"]=IDs; pad[\"Aspect\"]=ASP\n",
    "    return pad\n",
    "\n",
    "train_loader=DataLoader(train_ds,batch_size=8,shuffle=True,collate_fn=collate)\n",
    "dev_loader=DataLoader(dev_ds,batch_size=16,shuffle=False,collate_fn=collate)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Dev batches:\", len(dev_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b1d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base=AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.reg=nn.Linear(768,2)\n",
    "    def forward(self,i,m):\n",
    "        out=self.base(input_ids=i,attention_mask=m)\n",
    "        cls=out.last_hidden_state[:,0]\n",
    "        return self.reg(cls)\n",
    "\n",
    "model=Model().to(device)\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=2e-5)\n",
    "loss_fn=nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45fc512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2339e9eaffb0429a82525445179b2a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.8658702969551086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b15e3f7b884e178811aa02be8944d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.4105672240257263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a39599d6f5941429881cfd9765d2a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.06414695084095001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e80fdafeb461496588b5595bc3a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.23486857116222382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa7e29ebb154ade9f827b79c7524836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.31302016973495483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS=5\n",
    "print(\"Training start...\")\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    for b in tqdm(train_loader):\n",
    "        ids=b[\"input_ids\"].to(device)\n",
    "        mask=b[\"attention_mask\"].to(device)\n",
    "        y=b[\"labels\"].to(device)\n",
    "\n",
    "        pred=model(ids,mask)\n",
    "        loss=loss_fn(pred,y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print(\"Epoch\",ep+1,\"loss:\",loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf1d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11139cff5ec640f681e8044df92c53e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Inference...\")\n",
    "\n",
    "model.eval()\n",
    "preds=[]\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(dev_loader):\n",
    "        ids=b[\"input_ids\"].to(device)\n",
    "        mask=b[\"attention_mask\"].to(device)\n",
    "        logits=model(ids,mask).cpu().numpy()\n",
    "        for i,(ID,A) in enumerate(zip(b[\"ID\"],b[\"Aspect\"])):\n",
    "            v,a=logits[i]\n",
    "            preds.append((ID,A,f\"{v:.2f}#{a:.2f}\"))\n",
    "\n",
    "print(\"Total predictions:\", len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c9bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pred_eng_laptop.jsonl\n",
      "Exists: True\n",
      "Size: 22079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT='pred_eng_laptop.jsonl'\n",
    "sub={}\n",
    "for ID,ASP,VA in preds:\n",
    "    sub.setdefault(ID,[]).append({\"Aspect\":ASP,\"VA\":VA})\n",
    "\n",
    "with open(OUT,'w',encoding='utf8') as f:\n",
    "    for ex in dev_json:\n",
    "        f.write(json.dumps({\"ID\":ex[\"ID\"],\"Aspect_VA\":sub.get(ex[\"ID\"],[])})+\"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT)\n",
    "print(\"Exists:\", os.path.exists(OUT))\n",
    "print(\"Size:\", os.path.getsize(OUT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
