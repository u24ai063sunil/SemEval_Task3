{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXQcm8kFUC4m",
        "outputId": "54f9ffb5-4b20-4565-f286-bd6b9a0ff6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch scikit-learn tqdm emoji seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WE79epeKUT_8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import emoji\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kWuAii7JUVf_"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = \"/content/eng_laptop_train_alltasks.jsonl\"\n",
        "TEST_PATH  = \"/content/eng_laptop_test_task2.jsonl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FngePpwZUfpd",
        "outputId": "c16c3512-7876-43e2-e047-915a5cf817cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train reviews: 4076\n",
            "Test reviews : 1000\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_raw = load_jsonl(TRAIN_PATH)\n",
        "test_raw  = load_jsonl(TEST_PATH)\n",
        "\n",
        "print(\"Train reviews:\", len(train_raw))\n",
        "print(\"Test reviews :\", len(test_raw))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iUuXHFFBUykx"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Uf5fi4tU1ty"
      },
      "outputs": [],
      "source": [
        "def build_bio(text, aspects, opinions):\n",
        "    tokens = text.split()\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    def mark(span, tag):\n",
        "        span_toks = span.split()\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i:i+len(span_toks)] == span_toks:\n",
        "                labels[i] = f\"B-{tag}\"\n",
        "                for j in range(1, len(span_toks)):\n",
        "                    labels[i+j] = f\"I-{tag}\"\n",
        "\n",
        "    for a in aspects:\n",
        "        mark(a, \"ASP\")\n",
        "    for o in opinions:\n",
        "        mark(o, \"OPN\")\n",
        "\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvmknagnU7-P",
        "outputId": "358fddd6-42db-4615-8dd9-2af1073eb310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 3260\n",
            "Val samples  : 816\n"
          ]
        }
      ],
      "source": [
        "def preprocess_train(data):\n",
        "    samples = []\n",
        "    for item in data:\n",
        "        text = clean_text(item[\"Text\"])\n",
        "        aspects, opinions = [], []\n",
        "\n",
        "        for q in item[\"Quadruplet\"]:\n",
        "            if q[\"Aspect\"] != \"NULL\":\n",
        "                aspects.append(q[\"Aspect\"])\n",
        "                opinions.append(q[\"Opinion\"])\n",
        "\n",
        "        labels = build_bio(text, aspects, opinions)\n",
        "        samples.append({\"text\": text, \"labels\": labels})\n",
        "\n",
        "    return samples\n",
        "\n",
        "train_data = preprocess_train(train_raw)\n",
        "train_data, val_data = train_test_split(\n",
        "    train_data, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_data))\n",
        "print(\"Val samples  :\", len(val_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mMDPvf5aU_E3"
      },
      "outputs": [],
      "source": [
        "test_data = [\n",
        "    {\"id\": x[\"ID\"], \"text\": clean_text(x[\"Text\"])}\n",
        "    for x in test_raw\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9XkRdwdZVD9s"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "label2id = {\"O\":0, \"B-ASP\":1, \"I-ASP\":2, \"B-OPN\":3, \"I-OPN\":4}\n",
        "id2label = {v:k for k,v in label2id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "QIRq3lnOVHZz"
      },
      "outputs": [],
      "source": [
        "class DimASTEDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        enc = tokenizer(\n",
        "            item[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = [label2id[l] for l in item[\"labels\"]]\n",
        "        labels += [0] * (128 - len(labels))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(labels[:128])\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "FAtbXSQVVNSe"
      },
      "outputs": [],
      "source": [
        "class DimASTEModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.classifier = torch.nn.Linear(768, len(label2id))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids, attention_mask)\n",
        "        return self.classifier(out.last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4PnB9XCVQh3",
        "outputId": "07a54ed5-492f-4dab-fab0-c8e2c271e4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 408/408 [01:25<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | BIO Loss: 0.4547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:23<00:00,  4.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | BIO Loss: 0.2638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | BIO Loss: 0.1960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | BIO Loss: 0.1450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | BIO Loss: 0.1193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | BIO Loss: 0.0970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | BIO Loss: 0.0778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 408/408 [01:22<00:00,  4.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | BIO Loss: 0.0719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DimASTEModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "class_weights = torch.tensor(\n",
        "    [0.02, 1.5, 1.5, 1.5, 1.5],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    DimASTEDataset(train_data),\n",
        "    batch_size=8,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "for epoch in range(8):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        logits = model(ids, mask)\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            logits.view(-1, len(label2id)),\n",
        "            labels.view(-1),\n",
        "            weight=class_weights\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | BIO Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZNMrwQYnBJT",
        "outputId": "ed1bba5c-ed05-48cf-ecc5-661df0900c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TrSqT5fYnK_X"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aspect_pos_ok(span, doc):\n",
        "    span = span.lower()\n",
        "    for token in doc:\n",
        "        if token.text.lower() in span.split():\n",
        "            # Always allow proper nouns (brands, models)\n",
        "            if token.pos_ == \"PROPN\":\n",
        "                return True\n",
        "            # Allow nouns and verbs as before\n",
        "            if token.pos_ in {\"NOUN\", \"VERB\"}:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def opinion_pos_ok(span, doc):\n",
        "    span = span.lower()\n",
        "    for token in doc:\n",
        "        if token.text.lower() in span.split():\n",
        "            if token.pos_ in {\"ADJ\", \"ADV\"}:\n",
        "                return True\n",
        "            if token.pos_ == \"VERB\" and token.dep_ not in {\"aux\", \"auxpass\"}:\n",
        "                return True\n",
        "    return False\n",
        "\n"
      ],
      "metadata": {
        "id": "LaiwTSzEZNPE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "lFkF_dXCVT3V"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = set(\"\"\"\n",
        "a an the for of to in on and but or with as at by from\n",
        "\"\"\".split())\n",
        "\n",
        "\n",
        "def valid_aspect_basic(a):\n",
        "    a = a.strip()\n",
        "    if len(a) < 2:\n",
        "        return False\n",
        "    if not any(c.isalpha() for c in a):\n",
        "        return False\n",
        "    if a.lower() in STOPWORDS:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def extract_triplets(text, labels):\n",
        "    tokens = text.split()\n",
        "\n",
        "    aspects = []\n",
        "    opinions = []\n",
        "\n",
        "    cur = []\n",
        "    cur_type = None\n",
        "\n",
        "    # ---- BIO span extraction (with I-without-B recovery) ----\n",
        "    for t, l in zip(tokens, labels):\n",
        "        if l.startswith(\"B-\"):\n",
        "            if cur:\n",
        "                if cur_type == \"ASP\":\n",
        "                    aspects.append(\" \".join(cur))\n",
        "                elif cur_type == \"OPN\":\n",
        "                    opinions.append(\" \".join(cur))\n",
        "            cur = [t]\n",
        "            cur_type = l[2:]\n",
        "\n",
        "        elif l.startswith(\"I-\"):\n",
        "            if cur_type == l[2:]:\n",
        "                cur.append(t)\n",
        "            else:\n",
        "                cur = [t]\n",
        "                cur_type = l[2:]\n",
        "\n",
        "        else:\n",
        "            if cur:\n",
        "                if cur_type == \"ASP\":\n",
        "                    aspects.append(\" \".join(cur))\n",
        "                elif cur_type == \"OPN\":\n",
        "                    opinions.append(\" \".join(cur))\n",
        "            cur = []\n",
        "            cur_type = None\n",
        "\n",
        "    if cur:\n",
        "        if cur_type == \"ASP\":\n",
        "            aspects.append(\" \".join(cur))\n",
        "        elif cur_type == \"OPN\":\n",
        "            opinions.append(\" \".join(cur))\n",
        "\n",
        "    if not aspects or not opinions:\n",
        "        return []\n",
        "\n",
        "    # ---- spaCy processing ----\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # ---- Aspect filtering: noun OR verb ----\n",
        "    aspects = [\n",
        "        a for a in aspects\n",
        "        if valid_aspect_basic(a) and aspect_pos_ok(a, doc)\n",
        "    ]\n",
        "\n",
        "    # ---- Opinion filtering: adjective OR adverb ----\n",
        "    opinions = [\n",
        "        o for o in opinions\n",
        "        if opinion_pos_ok(o, doc)\n",
        "    ]\n",
        "\n",
        "    if not aspects or not opinions:\n",
        "        return []\n",
        "\n",
        "    # ---- Nearest-aspect pairing (stable heuristic) ----\n",
        "    triplets = []\n",
        "\n",
        "    for o in opinions:\n",
        "        o_pos = text.find(o)\n",
        "        if o_pos == -1:\n",
        "            continue\n",
        "\n",
        "        closest_a = None\n",
        "        min_dist = float(\"inf\")\n",
        "\n",
        "        for a in aspects:\n",
        "            a_pos = text.find(a)\n",
        "            if a_pos == -1:\n",
        "                continue\n",
        "            dist = abs(o_pos - a_pos)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                closest_a = a\n",
        "\n",
        "        if closest_a is not None:\n",
        "            triplets.append({\n",
        "                \"Aspect\": closest_a,\n",
        "                \"Opinion\": o\n",
        "            })\n",
        "\n",
        "    return triplets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xFSbhYkKVjF7"
      },
      "outputs": [],
      "source": [
        "class AspectVAModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.regressor = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids, attention_mask)\n",
        "        pooled = out.last_hidden_state[:,0,:]\n",
        "        return self.regressor(pooled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "O3bWLTslX3AD"
      },
      "outputs": [],
      "source": [
        "def preprocess_va(data):\n",
        "    rows = []\n",
        "    for item in data:\n",
        "        text = clean_text(item[\"Text\"])\n",
        "        for q in item[\"Quadruplet\"]:\n",
        "            if q[\"Aspect\"] != \"NULL\":\n",
        "                v, a = map(float, q[\"VA\"].split(\"#\"))\n",
        "                rows.append({\n",
        "                    \"text\": f\"<ASP>{q['Aspect']}</ASP> {text}\",\n",
        "                    \"va\": [v, a]\n",
        "                })\n",
        "    return rows\n",
        "\n",
        "va_data = preprocess_va(train_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "61OJ-DDWVr5h"
      },
      "outputs": [],
      "source": [
        "class VADataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        enc = tokenizer(\n",
        "            item[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(item[\"va\"], dtype=torch.float)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImAfKHnWVw2w",
        "outputId": "20530c34-c9ab-485c-a58b-81c2ab5d6e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 283/283 [01:37<00:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 1 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283/283 [01:36<00:00,  2.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 2 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283/283 [01:36<00:00,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 3 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "va_model = AspectVAModel().to(device)\n",
        "va_optimizer = torch.optim.AdamW(va_model.parameters(), lr=2e-5)\n",
        "\n",
        "va_loader = DataLoader(VADataset(va_data), batch_size=16, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    va_model.train()\n",
        "    for batch in tqdm(va_loader):\n",
        "        va_optimizer.zero_grad()\n",
        "        preds = va_model(\n",
        "            batch[\"input_ids\"].to(device),\n",
        "            batch[\"attention_mask\"].to(device)\n",
        "        )\n",
        "        loss = torch.nn.functional.smooth_l1_loss(\n",
        "            preds, batch[\"labels\"].to(device)\n",
        "        )\n",
        "        loss.backward()\n",
        "        va_optimizer.step()\n",
        "\n",
        "    print(f\"VA Epoch {epoch+1} done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ4OW0x5WGRp",
        "outputId": "d3263320-1b67-4d2c-ebf1-95ad66d97c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred_eng_laptop.jsonl generated ✅\n"
          ]
        }
      ],
      "source": [
        "def predict_va(text, aspect):\n",
        "    inp = f\"<ASP>{aspect}</ASP> {text}\"\n",
        "    enc = tokenizer(inp, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        va = va_model(\n",
        "            enc[\"input_ids\"].to(device),\n",
        "            enc[\"attention_mask\"].to(device)\n",
        "        )[0].cpu().numpy()\n",
        "    return np.clip(va, 1.0, 9.0)\n",
        "\n",
        "model.eval()\n",
        "va_model.eval()\n",
        "\n",
        "with open(\"pred_eng_laptop.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in test_data:\n",
        "        enc = tokenizer(item[\"text\"], return_tensors=\"pt\")\n",
        "        logits = model(\n",
        "            enc[\"input_ids\"].to(device),\n",
        "            enc[\"attention_mask\"].to(device)\n",
        "        )\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "        labels = []\n",
        "        for p in probs:\n",
        "            if p[label2id[\"B-ASP\"]] > 0.30:\n",
        "                labels.append(\"B-ASP\")\n",
        "            elif p[label2id[\"B-OPN\"]] > 0.30:\n",
        "                labels.append(\"B-OPN\")\n",
        "            else:\n",
        "                labels.append(\"O\")\n",
        "\n",
        "        triplets = extract_triplets(item[\"text\"], labels)\n",
        "\n",
        "        out = []\n",
        "        for t in triplets:\n",
        "            va = predict_va(item[\"text\"], t[\"Aspect\"])\n",
        "            out.append({\n",
        "                \"Aspect\": t[\"Aspect\"],\n",
        "                \"Opinion\": t[\"Opinion\"],\n",
        "                \"VA\": f\"{va[0]:.2f}#{va[1]:.2f}\"\n",
        "            })\n",
        "\n",
        "        f.write(json.dumps({\"ID\": item[\"id\"], \"Triplet\": out}) + \"\\n\")\n",
        "\n",
        "print(\"pred_eng_laptop.jsonl generated ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiYueru_XdSN",
        "outputId": "15fb00a4-ca39-4171-d0a2-83f901a5d6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Subtask-2 Validation Performance (cF1) ===\n",
            "cPrecision : 0.2437\n",
            "cRecall    : 0.0514\n",
            "cF1        : 0.0849\n",
            "\n",
            "Triplet coverage:\n",
            "Predicted triplets: 949\n",
            "Gold triplets     : 4500\n",
            "Recall ceiling    : 0.21088888888888888\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# cF1 VALIDATION CELL\n",
        "# =========================\n",
        "\n",
        "D_MAX = 128.0\n",
        "\n",
        "def va_distance(pred_va, gold_va):\n",
        "    return ((pred_va[0]-gold_va[0])**2 + (pred_va[1]-gold_va[1])**2) / D_MAX\n",
        "\n",
        "def ctp(pred_va, gold_va):\n",
        "    return max(0.0, 1.0 - va_distance(pred_va, gold_va))\n",
        "\n",
        "\n",
        "# ---- Build GOLD triplets from train_raw ----\n",
        "gold_triplets = {}\n",
        "\n",
        "for item in train_raw:\n",
        "    text = clean_text(item[\"Text\"])\n",
        "    trips = []\n",
        "\n",
        "    for q in item[\"Quadruplet\"]:\n",
        "        if q[\"Aspect\"] == \"NULL\":\n",
        "            continue\n",
        "        v, a = map(float, q[\"VA\"].split(\"#\"))\n",
        "        trips.append({\n",
        "            \"Aspect\": q[\"Aspect\"].strip(),\n",
        "            \"Opinion\": q[\"Opinion\"].strip(),\n",
        "            \"VA\": np.array([v, a])\n",
        "        })\n",
        "\n",
        "    gold_triplets[text] = trips\n",
        "\n",
        "\n",
        "# ---- Predict triplets on VAL data ----\n",
        "def predict_triplets_val(val_data):\n",
        "    preds = {}\n",
        "\n",
        "    model.eval()\n",
        "    va_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in val_data:\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "            logits = model(\n",
        "                enc[\"input_ids\"].to(device),\n",
        "                enc[\"attention_mask\"].to(device)\n",
        "            )\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)[0]\n",
        "            labels = []\n",
        "\n",
        "            for p in probs:\n",
        "                if p[label2id[\"B-ASP\"]] > 0.25:\n",
        "                    labels.append(\"B-ASP\")\n",
        "                elif p[label2id[\"B-OPN\"]] > 0.25:\n",
        "                    labels.append(\"B-OPN\")\n",
        "                else:\n",
        "                    labels.append(\"O\")\n",
        "\n",
        "            triplets = extract_triplets(text, labels)\n",
        "\n",
        "            out = []\n",
        "            for t in triplets:\n",
        "                va = predict_va(text, t[\"Aspect\"])\n",
        "                out.append({\n",
        "                    \"Aspect\": t[\"Aspect\"],\n",
        "                    \"Opinion\": t[\"Opinion\"],\n",
        "                    \"VA\": va\n",
        "                })\n",
        "\n",
        "            preds[text] = out\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "pred_triplets = predict_triplets_val(val_data)\n",
        "\n",
        "\n",
        "# ---- Compute official cF1 ----\n",
        "def compute_cF1(preds, golds):\n",
        "    ctp_sum = 0.0\n",
        "    pred_count = 0\n",
        "    gold_count = 0\n",
        "\n",
        "    for text in golds:\n",
        "        gold_trips = golds[text]\n",
        "        pred_trips = preds.get(text, [])\n",
        "\n",
        "        gold_count += len(gold_trips)\n",
        "        pred_count += len(pred_trips)\n",
        "\n",
        "        used = set()\n",
        "        for p in pred_trips:\n",
        "            for i, g in enumerate(gold_trips):\n",
        "                if i in used:\n",
        "                    continue\n",
        "                if p[\"Aspect\"] == g[\"Aspect\"] and p[\"Opinion\"] == g[\"Opinion\"]:\n",
        "                    ctp_sum += ctp(p[\"VA\"], g[\"VA\"])\n",
        "                    used.add(i)\n",
        "                    break\n",
        "\n",
        "    if pred_count == 0 or gold_count == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    cP = ctp_sum / pred_count\n",
        "    cR = ctp_sum / gold_count\n",
        "    cF1 = 2 * cP * cR / (cP + cR) if (cP + cR) > 0 else 0.0\n",
        "\n",
        "    return cP, cR, cF1\n",
        "\n",
        "\n",
        "# ---- Run evaluation ----\n",
        "cP, cR, cF1 = compute_cF1(pred_triplets, gold_triplets)\n",
        "\n",
        "print(\"=== Subtask-2 Validation Performance (cF1) ===\")\n",
        "print(f\"cPrecision : {cP:.4f}\")\n",
        "print(f\"cRecall    : {cR:.4f}\")\n",
        "print(f\"cF1        : {cF1:.4f}\")\n",
        "\n",
        "# ---- Debug coverage ----\n",
        "total_pred = sum(len(v) for v in pred_triplets.values())\n",
        "total_gold = sum(len(v) for v in gold_triplets.values())\n",
        "\n",
        "print(\"\\nTriplet coverage:\")\n",
        "print(\"Predicted triplets:\", total_pred)\n",
        "print(\"Gold triplets     :\", total_gold)\n",
        "print(\"Recall ceiling    :\", total_pred / total_gold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BcGXpg7dg1iW",
        "outputId": "f7a89158-22f5-436f-9afe-08d4e76fe6dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c32237a-4b35-42a4-9ac6-0f7e8dd04f2f\", \"pred_eng_laptop.jsonl\", 150046)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"pred_eng_laptop.jsonl\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}