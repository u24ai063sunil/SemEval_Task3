{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXQcm8kFUC4m",
        "outputId": "ece16da9-86f3-449c-e88b-ce5ddbe0fea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch scikit-learn tqdm emoji seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi ipadic unidic_lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_G-Fccu4Yc8",
        "outputId": "1a765089-ef02-4e04-d5bc-7cc11857eaf9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Collecting unidic_lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic_lite\n",
            "  Building wheel for unidic_lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic_lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=ab11e22df6eb506b9974fe918bbbb24cfdf6f7822c6429d0ecea7c2b331c3429\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/1f/0f/4d43887e5476d956fae828ee9b6687becd5544d68b51ed633d\n",
            "Successfully built unidic_lite\n",
            "Installing collected packages: unidic_lite\n",
            "Successfully installed unidic_lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WE79epeKUT_8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import emoji\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kWuAii7JUVf_"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = \"/content/jpn_hotel_train_alltasks.jsonl\"\n",
        "TEST_PATH  = \"/content/jpn_hotel_test_task2.jsonl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FngePpwZUfpd",
        "outputId": "de50317c-d547-42ce-b63a-67b192ce5ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train reviews: 1600\n",
            "Test reviews : 800\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_raw = load_jsonl(TRAIN_PATH)\n",
        "test_raw  = load_jsonl(TEST_PATH)\n",
        "\n",
        "print(\"Train reviews:\", len(train_raw))\n",
        "print(\"Test reviews :\", len(test_raw))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iUuXHFFBUykx"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "label2id = {\"O\":0, \"B-ASP\":1, \"I-ASP\":2, \"B-OPN\":3, \"I-OPN\":4}\n",
        "id2label = {v:k for k,v in label2id.items()}"
      ],
      "metadata": {
        "id": "WT0TIhRy4uOR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Uf5fi4tU1ty"
      },
      "outputs": [],
      "source": [
        "def build_bio(tokens, aspects, opinions):\n",
        "  labels = [\"O\"] * len(tokens)\n",
        "\n",
        "\n",
        "def mark(span, tag):\n",
        "  span_tokens = tokenizer.tokenize(span)\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i:i+len(span_tokens)] == span_tokens:\n",
        "      labels[i] = f\"B-{tag}\"\n",
        "    for j in range(1, len(span_tokens)):\n",
        "      labels[i+j] = f\"I-{tag}\"\n",
        "\n",
        "\n",
        "    for a in aspects:\n",
        "      mark(a, \"ASP\")\n",
        "    for o in opinions:\n",
        "      mark(o, \"OPN\")\n",
        "\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvmknagnU7-P",
        "outputId": "5069a6c5-3c3c-4044-bdbd-9664c948d701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 1280\n",
            "Val samples : 320\n"
          ]
        }
      ],
      "source": [
        "def preprocess_train(data):\n",
        "    samples = []\n",
        "    for item in data:\n",
        "        text = clean_text(item[\"Text\"])\n",
        "        aspects, opinions = [], []\n",
        "\n",
        "        for q in item[\"Quadruplet\"]:\n",
        "            if q[\"Aspect\"] != \"NULL\":\n",
        "                aspects.append(q[\"Aspect\"])\n",
        "                opinions.append(q[\"Opinion\"])\n",
        "\n",
        "        labels = build_bio(text, aspects, opinions)\n",
        "        samples.append({\"text\": text, \"labels\": labels})\n",
        "\n",
        "    return samples\n",
        "\n",
        "train_data = preprocess_train(train_raw)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(\"Train samples:\", len(train_data))\n",
        "print(\"Val samples :\", len(val_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mMDPvf5aU_E3"
      },
      "outputs": [],
      "source": [
        "test_data = [\n",
        "    {\"id\": x[\"ID\"], \"text\": clean_text(x[\"Text\"])}\n",
        "    for x in test_raw\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIRq3lnOVHZz"
      },
      "outputs": [],
      "source": [
        "class DimASTEDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        enc = tokenizer(\n",
        "            item[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = [label2id[l] for l in item[\"labels\"]]\n",
        "        labels += [0] * (128 - len(labels))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(labels[:128])\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [CELL 7] SAFE BIO construction\n",
        "\n",
        "def build_bio(tokens, aspects, opinions):\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    def mark(span, tag):\n",
        "        if not span:\n",
        "            return\n",
        "        span_tokens = tokenizer.tokenize(span)\n",
        "        if not span_tokens:\n",
        "            return\n",
        "        for i in range(len(tokens) - len(span_tokens) + 1):\n",
        "            if tokens[i:i+len(span_tokens)] == span_tokens:\n",
        "                labels[i] = f\"B-{tag}\"\n",
        "                for j in range(1, len(span_tokens)):\n",
        "                    labels[i+j] = f\"I-{tag}\"\n",
        "\n",
        "    for a in aspects:\n",
        "        mark(a, \"ASP\")\n",
        "    for o in opinions:\n",
        "        mark(o, \"OPN\")\n",
        "\n",
        "    return labels\n",
        "\n",
        "# %% [CELL 8] Preprocess train (SAFE)\n",
        "\n",
        "def preprocess_train(data):\n",
        "    rows = []\n",
        "    for it in data:\n",
        "        text = clean_text(it['Text'])\n",
        "        aspects, opinions = [], []\n",
        "        for q in it['Quadruplet']:\n",
        "            if q['Aspect'] != 'NULL':\n",
        "                aspects.append(q['Aspect'])\n",
        "                opinions.append(q['Opinion'])\n",
        "\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        if len(tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        labels = build_bio(tokens, aspects, opinions)\n",
        "        if labels is None or len(labels) != len(tokens):\n",
        "            labels = [\"O\"] * len(tokens)\n",
        "\n",
        "        rows.append({'text': text, 'labels': labels})\n",
        "    return rows\n",
        "\n",
        "train_data = preprocess_train(train_raw)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "print(\"Train samples:\", len(train_data), \"Val samples:\", len(val_data))\n",
        "\n",
        "# %% [CELL 9] Test data\n",
        "test_data = [{'id':x['ID'], 'text':clean_text(x['Text'])} for x in test_raw]\n",
        "\n",
        "# %% [CELL 10] Dataset (SAFE)\n",
        "class DimASTEDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.data[idx]\n",
        "        enc = tokenizer(it['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        labels = it.get('labels', []) or []\n",
        "        label_ids = [label2id.get(l, 0) for l in labels]\n",
        "        label_ids += [0] * (128 - len(label_ids))\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label_ids[:128])\n",
        "        }\n",
        "\n",
        "# %% [CELL 11] BIO model\n",
        "class DimASTEModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.classifier = torch.nn.Linear(768, len(label2id))\n",
        "    def forward(self, ids, mask):\n",
        "        return self.classifier(self.encoder(ids, mask).last_hidden_state)\n",
        "\n",
        "# %% [CELL 12] Train BIO\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DimASTEModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "class_weights = torch.tensor([0.02,1.5,1.5,1.5,1.5], device=device)\n",
        "loader = DataLoader(DimASTEDataset(train_data), batch_size=8, shuffle=True)\n",
        "BIO_EPOCHS = 8\n",
        "for e in range(BIO_EPOCHS):\n",
        "    model.train(); total = 0\n",
        "    for b in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(b['input_ids'].to(device), b['attention_mask'].to(device))\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            logits.view(-1, len(label2id)),\n",
        "            b['labels'].to(device).view(-1),\n",
        "            weight=class_weights\n",
        "        )\n",
        "        loss.backward(); optimizer.step()\n",
        "        total += loss.item()\n",
        "    print(f\"BIO Epoch {e+1}/{BIO_EPOCHS} | Loss: {total/len(loader):.4f}\")\n",
        "\n",
        "# %% [CELL 13] Decode with smoothing\n",
        "\n",
        "def decode_with_smoothing(probs, b_th, i_th):\n",
        "    labels = []\n",
        "    prev = 'O'\n",
        "    for p in probs:\n",
        "        if p[label2id['B-ASP']] > b_th:\n",
        "            lab = 'B-ASP'\n",
        "        elif p[label2id['B-OPN']] > b_th:\n",
        "            lab = 'B-OPN'\n",
        "        elif prev.startswith('B') and p[label2id['I-'+prev[2:]]] > i_th:\n",
        "            lab = 'I-' + prev[2:]\n",
        "        else:\n",
        "            lab = 'O'\n",
        "        labels.append(lab)\n",
        "        prev = lab\n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "# %% [CELL 14] Extract triplets (NEAREST-PAIRING – CRITICAL)\n",
        "\n",
        "def extract_triplets(text, labels):\n",
        "    toks = tokenizer.tokenize(text)\n",
        "\n",
        "    aspects = []\n",
        "    opinions = []\n",
        "\n",
        "    cur = []\n",
        "    cur_type = None\n",
        "\n",
        "    # ---- BIO span extraction ----\n",
        "    for tok, lab in zip(toks, labels):\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if cur:\n",
        "                if cur_type == \"ASP\":\n",
        "                    aspects.append(\"\".join(cur))\n",
        "                elif cur_type == \"OPN\":\n",
        "                    opinions.append(\"\".join(cur))\n",
        "            cur = [tok]\n",
        "            cur_type = lab[2:]\n",
        "\n",
        "        elif lab.startswith(\"I-\") and cur_type == lab[2:]:\n",
        "            cur.append(tok)\n",
        "\n",
        "        else:\n",
        "            if cur:\n",
        "                if cur_type == \"ASP\":\n",
        "                    aspects.append(\"\".join(cur))\n",
        "                elif cur_type == \"OPN\":\n",
        "                    opinions.append(\"\".join(cur))\n",
        "            cur = []\n",
        "            cur_type = None\n",
        "\n",
        "    if cur:\n",
        "        if cur_type == \"ASP\":\n",
        "            aspects.append(\"\".join(cur))\n",
        "        elif cur_type == \"OPN\":\n",
        "            opinions.append(\"\".join(cur))\n",
        "\n",
        "    if not aspects or not opinions:\n",
        "        return []\n",
        "\n",
        "    # ---- NEAREST PAIRING ----\n",
        "    pairs = []\n",
        "    for o in opinions:\n",
        "        o_pos = text.find(o)\n",
        "        if o_pos == -1:\n",
        "            continue\n",
        "\n",
        "        best_a = None\n",
        "        best_dist = 1e9\n",
        "\n",
        "        for a in aspects:\n",
        "            a_pos = text.find(a)\n",
        "            if a_pos == -1:\n",
        "                continue\n",
        "\n",
        "            dist = abs(o_pos - a_pos)\n",
        "            if dist < best_dist:\n",
        "                best_dist = dist\n",
        "                best_a = a\n",
        "\n",
        "        if best_a is not None:\n",
        "            pairs.append({\"Aspect\": best_a, \"Opinion\": o})\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# %% [CELL 15] Threshold tuning (VAL)\n",
        "best_th, best_f1 = 0.3, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for th in np.arange(0.15, 0.45, 0.05):\n",
        "        tp = fp = fn = 0\n",
        "        for it in val_data:\n",
        "            enc = tokenizer(it['text'], return_tensors='pt', truncation=True)\n",
        "            probs = torch.softmax(model(enc['input_ids'].to(device), enc['attention_mask'].to(device))[0], -1)\n",
        "            labs = decode_with_smoothing(probs, th, th-0.05)\n",
        "            preds = {(p['Aspect'], p['Opinion']) for p in extract_triplets(it['text'], labs)}\n",
        "            gold = set()\n",
        "            for r in train_raw:\n",
        "                if clean_text(r['Text']) == it['text']:\n",
        "                    gold = {(q['Aspect'], q['Opinion']) for q in r['Quadruplet'] if q['Aspect']!='NULL'}\n",
        "                    break\n",
        "            tp += len(preds & gold)\n",
        "            fp += len(preds - gold)\n",
        "            fn += len(gold - preds)\n",
        "        f1 = 2*tp/(2*tp+fp+fn+1e-9)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "print(\"Best threshold:\", best_th)\n",
        "\n",
        "# %% [CELL 16] VA model\n",
        "class AspectVAModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.regressor = torch.nn.Linear(768, 2)\n",
        "    def forward(self, ids, mask):\n",
        "        return self.regressor(self.encoder(ids, mask).last_hidden_state[:,0])\n",
        "\n",
        "# %% [CELL 17] VA data\n",
        "def preprocess_va(data):\n",
        "    rows = []\n",
        "    for it in data:\n",
        "        text = clean_text(it['Text'])\n",
        "        for q in it['Quadruplet']:\n",
        "            if q['Aspect'] != 'NULL':\n",
        "                v,a = map(float, q['VA'].split('#'))\n",
        "                rows.append({'text': f\"<ASP>{q['Aspect']}</ASP> {text}\", 'va': torch.tensor([v,a])})\n",
        "    return rows\n",
        "va_data = preprocess_va(train_raw)\n",
        "\n",
        "# %% [CELL 18] VA Dataset\n",
        "class VADataset(Dataset):\n",
        "    def __init__(self, data): self.data=data\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.data[idx]\n",
        "        enc = tokenizer(it['text'], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "            'labels': it['va']\n",
        "        }\n",
        "\n",
        "# %% [CELL 19] Train VA\n",
        "va_model = AspectVAModel().to(device)\n",
        "va_opt = torch.optim.AdamW(va_model.parameters(), lr=2e-5)\n",
        "va_loader = DataLoader(VADataset(va_data), batch_size=16, shuffle=True)\n",
        "VA_EPOCHS = 3\n",
        "for e in range(VA_EPOCHS):\n",
        "    va_model.train(); total = 0\n",
        "    for b in tqdm(va_loader):\n",
        "        va_opt.zero_grad()\n",
        "        preds = va_model(b['input_ids'].to(device), b['attention_mask'].to(device))\n",
        "        loss = torch.nn.functional.smooth_l1_loss(preds, b['labels'].to(device))\n",
        "        loss.backward(); va_opt.step(); total += loss.item()\n",
        "    print(f\"VA Epoch {e+1}/{VA_EPOCHS} | Loss: {total/len(va_loader):.4f}\")\n",
        "\n",
        "# %% [CELL 20] Final prediction JSONL\n",
        "model.eval(); va_model.eval()\n",
        "with open('pred_jpn_hotel.jsonl', 'w', encoding='utf-8') as f:\n",
        "    with torch.no_grad():\n",
        "        for it in test_data:\n",
        "            enc = tokenizer(it['text'], return_tensors='pt', truncation=True)\n",
        "            probs = torch.softmax(model(enc['input_ids'].to(device), enc['attention_mask'].to(device))[0], -1)\n",
        "            labs = decode_with_smoothing(probs, best_th, best_th-0.05)\n",
        "            trips = extract_triplets(it['text'], labs)\n",
        "            out = []\n",
        "            for t in trips:\n",
        "                enc2 = tokenizer(f\"<ASP>{t['Aspect']}</ASP> {it['text']}\", return_tensors='pt', truncation=True)\n",
        "                va = va_model(enc2['input_ids'].to(device), enc2['attention_mask'].to(device))[0].cpu().numpy()\n",
        "                va = np.clip(va, 1.0, 9.0)\n",
        "                out.append({'Aspect':t['Aspect'], 'Opinion':t['Opinion'], 'VA':f\"{va[0]:.2f}#{va[1]:.2f}\"})\n",
        "            f.write(json.dumps({'ID':it['id'], 'Triplet':out}, ensure_ascii=False)+'\\n')\n",
        "print('pred_jpn_hotel.jsonl generated ✅')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGdW6xg76WRa",
        "outputId": "255aeef8-2f5d-4cc5-c253-239323f8cd7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.45000000000000007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158/158 [00:58<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 1/3 | Loss: 0.5093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158/158 [00:57<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 2/3 | Loss: 0.1524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158/158 [00:58<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA Epoch 3/3 | Loss: 0.1019\n",
            "pred_jpn_hotel.jsonl generated ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BcGXpg7dg1iW",
        "outputId": "8aa84a4d-ab66-4a4a-d97a-e699cab44c47"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3284b83-1483-4167-a09b-4f2645cb8935\", \"pred_jpn_hotel.jsonl\", 117129)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"pred_jpn_hotel.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [OFFICIAL cF1 EVALUATION CELL – SUBTASK 2]\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# -------------------------\n",
        "# Official metric helpers\n",
        "# -------------------------\n",
        "D_MAX = 128.0\n",
        "\n",
        "def va_distance(pred_va, gold_va):\n",
        "    return ((pred_va[0] - gold_va[0])**2 + (pred_va[1] - gold_va[1])**2) / D_MAX\n",
        "\n",
        "def ctp(pred_va, gold_va):\n",
        "    return max(0.0, 1.0 - va_distance(pred_va, gold_va))\n",
        "\n",
        "# -------------------------\n",
        "# Build GOLD triplets (VAL)\n",
        "# -------------------------\n",
        "gold_triplets = {}\n",
        "\n",
        "for item in train_raw:\n",
        "    text = clean_text(item[\"Text\"])\n",
        "    trips = []\n",
        "\n",
        "    for q in item[\"Quadruplet\"]:\n",
        "        if q[\"Aspect\"] == \"NULL\":\n",
        "            continue\n",
        "        v, a = map(float, q[\"VA\"].split(\"#\"))\n",
        "        trips.append({\n",
        "            \"Aspect\": q[\"Aspect\"].strip(),\n",
        "            \"Opinion\": q[\"Opinion\"].strip(),\n",
        "            \"VA\": np.array([v, a])\n",
        "        })\n",
        "\n",
        "    gold_triplets[text] = trips\n",
        "\n",
        "# -------------------------\n",
        "# Predict triplets on VAL\n",
        "# -------------------------\n",
        "def predict_triplets_val(val_data):\n",
        "    preds = {}\n",
        "    model.eval()\n",
        "    va_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in val_data:\n",
        "            text = item[\"text\"]\n",
        "\n",
        "            enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "            logits = model(\n",
        "                enc[\"input_ids\"].to(device),\n",
        "                enc[\"attention_mask\"].to(device)\n",
        "            )\n",
        "\n",
        "            probs = torch.softmax(logits[0], dim=-1)\n",
        "            labels = decode_with_smoothing(probs, best_th, best_th - 0.05)\n",
        "            triplets = extract_triplets(text, labels)\n",
        "\n",
        "            out = []\n",
        "            for t in triplets:\n",
        "                enc2 = tokenizer(\n",
        "                    f\"<ASP>{t['Aspect']}</ASP> {text}\",\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True\n",
        "                )\n",
        "                va = va_model(\n",
        "                    enc2[\"input_ids\"].to(device),\n",
        "                    enc2[\"attention_mask\"].to(device)\n",
        "                )[0].cpu().numpy()\n",
        "\n",
        "                out.append({\n",
        "                    \"Aspect\": t[\"Aspect\"],\n",
        "                    \"Opinion\": t[\"Opinion\"],\n",
        "                    \"VA\": va\n",
        "                })\n",
        "\n",
        "            preds[text] = out\n",
        "\n",
        "    return preds\n",
        "\n",
        "pred_triplets = predict_triplets_val(val_data)\n",
        "\n",
        "# -------------------------\n",
        "# Compute official cF1\n",
        "# -------------------------\n",
        "def compute_cF1(preds, golds):\n",
        "    ctp_sum = 0.0\n",
        "    pred_count = 0\n",
        "    gold_count = 0\n",
        "\n",
        "    for text in golds:\n",
        "        gold_trips = golds[text]\n",
        "        pred_trips = preds.get(text, [])\n",
        "\n",
        "        gold_count += len(gold_trips)\n",
        "        pred_count += len(pred_trips)\n",
        "\n",
        "        used = set()\n",
        "        for p in pred_trips:\n",
        "            for i, g in enumerate(gold_trips):\n",
        "                if i in used:\n",
        "                    continue\n",
        "                if p[\"Aspect\"] == g[\"Aspect\"] and p[\"Opinion\"] == g[\"Opinion\"]:\n",
        "                    ctp_sum += ctp(p[\"VA\"], g[\"VA\"])\n",
        "                    used.add(i)\n",
        "                    break\n",
        "\n",
        "    if pred_count == 0 or gold_count == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    cP = ctp_sum / pred_count\n",
        "    cR = ctp_sum / gold_count\n",
        "    cF1 = 2 * cP * cR / (cP + cR) if (cP + cR) > 0 else 0.0\n",
        "\n",
        "    return cP, cR, cF1\n",
        "\n",
        "# -------------------------\n",
        "# Run evaluation\n",
        "# -------------------------\n",
        "cP, cR, cF1 = compute_cF1(pred_triplets, gold_triplets)\n",
        "\n",
        "print(\"=== OFFICIAL Subtask-2 Validation (cF1) ===\")\n",
        "print(f\"cPrecision : {cP:.4f}\")\n",
        "print(f\"cRecall    : {cR:.4f}\")\n",
        "print(f\"cF1        : {cF1:.4f}\")\n",
        "\n",
        "# Coverage diagnostics\n",
        "total_pred = sum(len(v) for v in pred_triplets.values())\n",
        "total_gold = sum(len(v) for v in gold_triplets.values())\n",
        "\n",
        "print(\"\\nTriplet coverage:\")\n",
        "print(\"Predicted triplets:\", total_pred)\n",
        "print(\"Gold triplets     :\", total_gold)\n",
        "print(\"Recall ceiling    :\", total_pred / max(1, total_gold))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HsjlHti76sJ",
        "outputId": "80c07ec0-2f72-41b0-f007-eca28483bee4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OFFICIAL Subtask-2 Validation (cF1) ===\n",
            "cPrecision : 0.2808\n",
            "cRecall    : 0.0540\n",
            "cF1        : 0.0906\n",
            "\n",
            "Triplet coverage:\n",
            "Predicted triplets: 483\n",
            "Gold triplets     : 2512\n",
            "Recall ceiling    : 0.19227707006369427\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}